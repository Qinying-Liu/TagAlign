_base_: "default.yml"
data:
  batch_size: 512
  dataset:
    train:
      - gcc3m
      - gcc12m
  train: 
      root_dir: [
                 # 'pcache://vilabpcacheproxyi-pool.cz50c.alipay.com:39999/mnt/bacba1e325b67e1aa48e7ff40f7e80bf/CC12M/images/',
                 'pcache://vilabpcacheproxyi-pool.cz50c.alipay.com:39999/mnt/',
                 # 'pcache://vilabpcacheproxyi-pool.cz50c.alipay.com:39999/mnt/',
                 # 'pcache://vilabpcacheproxyi-pool.cz50c.alipay.com:39999/mnt/',
                ]
      meta_file: [
                  '../data/CC12M/CC12M_meta.csv', # '../data/CC12M/CC12M-BLIP-caption/CC12M-BLIP-meta.csv',
                  # '../data/YFCC15M/YFCC15M_meta.csv',
                  # '../data/CC3M/cc3m_meta.csv', # '../data/CC3M/CC3M-BLIP-caption/CC3M-BLIP-meta.csv',
                 ]
      # read_from: petrel
      read_from: dir
      use_dali: True
      # batch_size: 256
      input_size: 224
      test_resize: 256

      image_reader:
          type: pil
      sampler:
          type: distributed_epoch
      transforms:
          type: STANDARD
      fseek: False

      # tag_file: [
      #   '../data/CC12M+CC3M_BASE+BLIP/meta_nouns_index_num1500_singular_CC12M_BLIP+BASE.json',
      #   # '../data/YFCC15M/YFCC15M-BLIP-tag/meta_nouns_index_top10000_singular_revised.json',
      #   '../data/CC12M+CC3M_BASE+BLIP/meta_nouns_index_num1500_singular_CC3M_BLIP+BASE.json',
      #   ]
      # num_tags: 4056 # 75105 # 50391 # 29349 # 12933 # 8884 # 5409 # 4056 # 3321
      # label_file: '../data/CC12M+CC3M_BASE+BLIP/label_embedding_num1500_singular_CC12M+CC3M_BLIP+BASE.pth'

      tag_file: [
        '../data/CC12M/label_embedding/meta_nouns_index_top100000.json', # '../data/CC12M/CC12M_meta_nouns_index.json', # '../data/CC12M/CC12M_meta_nouns_index.json',
        # '../data/YFCC15M/YFCC15M-BLIP-tag/meta_nouns_index_top10000_singular_revised.json',
        # '../data/CC3M/CC3M-tag/meta_nouns_index_top10000_singular_revised.json',
        ]
      num_tags: 100000 
      label_file: '../data/CC12M/label_embedding/label_embedding_top100000.pth' # '../data/CC12M/CC12M_textual_top10000_label_embedding.pth' # '../data/CC12M/CC12M_textual_top10000_label_embedding.pth'

      # tag_file: [
      #   '../data/CC12M/CC12M-BLIP+BASE-tag/meta_nouns_index_top10000_singular_BLIP+BASE.json',
      #   # '../data/YFCC15M/YFCC15M-BLIP-tag/meta_nouns_index_top10000_singular_revised.json',
      #   '../data/CC3M/CC3M-tag/meta_nouns_index_top10000_singular_BLIP+BASE.json',
      #   ]
      # num_tags: 10000 
      # label_file: '../data/CC12M/CC12M-BLIP+BASE-tag/label_embedding_singular_BLIP+BASE_10000.pth'
      

model:
  type: TCL
  clip_model: ../packs/ViT-B-16.pt  # NOTE only ViT-based backbones are supported.
  ie_freeze: 11  # index [1 ~ 12]
  ie_ignore_last_attn: false  # use MaskCLIP
  masker:
    type: Masker
    decoder:
      type: GDecoder
      double: true
      n_layers: 2
      kernel_size: 3
      act: gelu
      norm: ln
      text: false

    sim2mask:
      init_w: 10.0
      init_b: 0
      gumbel_tau: 1.0
      learnable_w: true
      learnable_b: false
      scale: 10.0
      ratio: 0.15

    # sim2mask:
    #   init_w: 1.0
    #   init_b: 0.0
    #   gumbel_tau: 1.0
    #   learnable: false

  # tcl
  tcl_w: 1.0 # contrastive loss
  tv_w: 0.0 # patch contrastive loss
  area_w: 1.0
  label_file: ${data.train.label_file}

train:
  ust_steps: 30000  # train decoder only in this steps
  total_steps: 50001
  warmup_steps: 25000
  base_lr: 1e-3
  weight_decay: 0.05
  min_lr: 4e-5
  clip_grad: 5.0
  optimizer:
    eps: 1e-6
  lr_scheduler:
    name: constant

evaluate:
  pamr: true
  bg_thresh: 0.4
  kp_w: 0.3

  eval_freq: 1000
  template: custom #identity_template # simple # custom # custom # full_imagenet_templates # simple
  task:
    - t_voc20
    # - t_context59

checkpoint:
  save_topk: 0

model_name: "release"
